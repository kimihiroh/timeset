{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79183d8b-f38e-4d6e-b37f-07c863d03864",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "* DCT-based performance comparison\n",
    "* Length-based performance comparison\n",
    "* #events/doc-based performance comparison\n",
    "* Topic-based performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f76462-4de4-4f8e-b8cf-ccf5b04ae6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c913a9-8752-414e-b4d0-b3202083cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../src/\")\n",
    "import utils_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972e184-5586-4129-9242-a43195fd0e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/preprocess/timeset-metadata/test.json\", \"r\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e41c42-4a07-4386-ad04-3065a26fa96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"health_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596fa5a8-cbbd-41b2-b8d9-507122ebbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05438b8a-9903-4a28-b294-185383cfbee1",
   "metadata": {},
   "source": [
    "plot with data axis seems to be not easy to interpret. just binary comparions\n",
    "* consider only Llama-2-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a0ecc-b1cc-4227-9a31-fa3d0ed617db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "def calculate_f1_per_doc(scores):\n",
    "    \n",
    "    filename2allscores = defaultdict(list)\n",
    "    for template_id, filename2scores in scores.items():\n",
    "        for filename, scores in filename2scores.items():\n",
    "            filename2allscores[filename].append(scores[\"weighted avg\"][\"f1-score\"])\n",
    "    \n",
    "    filename2score = defaultdict(float)\n",
    "    for filename, all_scores in filename2allscores.items():\n",
    "        filename2score[filename] = all_scores\n",
    "\n",
    "    return filename2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3acd6-61b1-41d3-a655-d7c8cfc9ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = Path(\"../output_score/comparison/\")\n",
    "scores = defaultdict(lambda: defaultdict(list))\n",
    "formulations = [\"nli\", \"pairwise\", \"mrc\", \"timeline\"]\n",
    "for formulation in formulations:\n",
    "    for filepath in dirpath.glob(f\"{formulation}/Llama-2-7b-hf*/*mention*eid*\"):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        if \"document-and-pair-wise-scores\" in data[\"individuals\"]:\n",
    "            for filename, score in calculate_f1_per_doc(data[\"individuals\"][\"document-and-pair-wise-scores\"]).items():\n",
    "                date_category = metadata[filename][\"date(binary)\"]\n",
    "                scores[formulation][date_category].extend(score)\n",
    "        else:\n",
    "            print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ac638-60fb-4321-8a74-6469b2a8dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_box_properties(plot_name, color_code, label):\n",
    "    for k, v in plot_name.items():\n",
    "        plt.setp(plot_name.get(k), color=color_code, alpha=0.7)\n",
    "    plt.plot([], c=color_code, label=label)\n",
    "    for median in plot_name[\"medians\"]:\n",
    "        median.set_color('black')\n",
    "\n",
    "ticks = [\"NLI\", \"Pairwise\", \"MRC\", \"Timeline\"]\n",
    "\n",
    "plt.figure(figsize=(4, 3))        \n",
    "for category, diff, color in zip([\"Old\", \"New\"], [0, 0.4], [\"#0072b2\", \"#d55e00\"]):\n",
    "    points_flat = [\n",
    "        scores[formulation][category]\n",
    "        for formulation in formulations\n",
    "    ]\n",
    "    positions_x = np.array(np.arange(len(points_flat)))*1.0-0.2+diff\n",
    "    plot = plt.boxplot(\n",
    "        points_flat,\n",
    "    \tpositions=positions_x, \n",
    "        widths=0.3,\n",
    "        showfliers=False,\n",
    "        patch_artist=True\n",
    "    )\n",
    "    define_box_properties(plot, color, category)\n",
    "\n",
    "plt.xticks(np.arange(0, len(ticks) * 1, 1), ticks)\n",
    "plt.grid(axis='y', color='g', linestyle=':', linewidth=0.3)\n",
    "plt.ylim(-0.05, 1.0)\n",
    "plt.legend()\n",
    "plt.box(False)\n",
    "plt.ylabel(\"F1\")\n",
    "# plt.xlabel(\"Formulation\")\n",
    "plt.savefig(f\"./figures/result_formulation_comparison_date.pdf\", format=\"pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc768c-1b7f-42bc-89b0-dbbf3667f24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51c4c6-230e-4d3e-a2a9-610e6561050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "def calculate_f1_per_doc(scores):\n",
    "    \n",
    "    filename2allscores = defaultdict(list)\n",
    "    for template_id, filename2scores in scores.items():\n",
    "        for filename, scores in filename2scores.items():\n",
    "            filename2allscores[filename].append(scores[\"weighted avg\"][\"f1-score\"])\n",
    "    \n",
    "    filename2score = defaultdict(float)\n",
    "    for filename, all_scores in filename2allscores.items():\n",
    "        filename2score[filename] = statistics.median(all_scores)\n",
    "\n",
    "    return filename2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc38d73-e731-43bb-8ead-c2c2a8d66702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirpath = Path(\"../output_score/comparison/\")\n",
    "scores = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "formulations = [\"nli\", \"pairwise\", \"mrc\", \"timeline\"]\n",
    "model_ids = [\"Llama-2-7b-hf\", \"flan-t5-xl\"]\n",
    "for model_id in model_ids:\n",
    "    for formulation in formulations:\n",
    "        for filepath in dirpath.glob(f\"{formulation}/{model_id}*/*mention*eid*\"):\n",
    "            with open(filepath, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            if \"document-and-pair-wise-scores\" in data[\"individuals\"]:\n",
    "                for filename, score in calculate_f1_per_doc(data[\"individuals\"][\"document-and-pair-wise-scores\"]).items():\n",
    "                    num = metadata[filename][\"#event\"]\n",
    "                    scores[model_id][formulation][num].append(score)\n",
    "            else:\n",
    "                print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1dde1-8038-4014-adee-0bd825a9bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['flan-t5-xl'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955692d-018f-4fd4-a741-22122f6cca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    (\"nli\", 'o', \"-\"), \n",
    "    (\"pairwise\", 'x', \"--\"), \n",
    "    (\"mrc\", 's', \":\"), \n",
    "    (\"timeline\", '^', \"-.\")\n",
    "]\n",
    "colors = ['#ffb000', '#648fff']\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for model_id, color in zip(model_ids, colors):\n",
    "    formulation2nums = defaultdict(list)\n",
    "    formulation2scores = defaultdict(list)\n",
    "    for formulation, num2scores in scores[model_id].items():\n",
    "        for num, _scores in num2scores.items():\n",
    "            formulation2nums[formulation].extend([num]*len(_scores))\n",
    "            formulation2scores[formulation].extend(_scores)\n",
    "    \n",
    "    # Plotting\n",
    "    for formulation, marker, linestyle in configs:\n",
    "        x, y = np.array(formulation2nums[formulation]), np.array(formulation2scores[formulation])\n",
    "        plot = plt.scatter(x, y, marker=marker, alpha=0.2, color=color)\n",
    "        x_elements = plot.get_offsets()[:, 0]\n",
    "        x = np.array(sorted(x_elements))\n",
    "        a, b = np.polyfit(x, y, 1)\n",
    "        plt.plot(x, a*x+b, linestyle=linestyle, color=color)\n",
    "\n",
    "plt.xlabel('#event')\n",
    "plt.ylabel('F1')\n",
    "\n",
    "plt.box(False)\n",
    "plt.grid(axis='y', color='g', linestyle=':', linewidth=0.3)\n",
    "plt.ylim(-0.0, 1.05)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='#ffb000', label='Llama 2 (7B)'),\n",
    "    Line2D([0], [0], color='#648fff', label='Flan-T5 (3B)'),\n",
    "    Line2D([0], [0], marker='o', color='grey', label='NLI', markerfacecolor='grey', linestyle='-'),\n",
    "    Line2D([0], [0], marker='x', color='grey', label='Pairwise', markerfacecolor='grey', linestyle='--'),\n",
    "    Line2D([0], [0], marker='s', color='grey', label='MRC', markerfacecolor='grey', linestyle=':'),\n",
    "    Line2D([0], [0], marker='^', color='grey', label='Timeline', markerfacecolor='grey', linestyle='-.'),\n",
    "]\n",
    "plt.legend(handles=legend_elements, ncol=3)\n",
    "\n",
    "plt.savefig(f\"./figures/result_formulation_comparison_num_event.pdf\", format=\"pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22850c9b-fb45-42f5-93c8-4bc926217eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569018c-202d-44f5-bf89-23dd41c62c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = Path(\"../output_score/comparison/\")\n",
    "scores = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "formulations = [\"nli\", \"pairwise\", \"mrc\", \"timeline\"]\n",
    "model_ids = [\"Llama-2-7b-hf\", \"flan-t5-xl\"]\n",
    "for model_id in model_ids:\n",
    "    for formulation in formulations:\n",
    "        for filepath in dirpath.glob(f\"{formulation}/{model_id}*/*mention*eid*\"):\n",
    "            with open(filepath, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            if \"document-and-pair-wise-scores\" in data[\"individuals\"]:\n",
    "                for filename, score in calculate_f1_per_doc(data[\"individuals\"][\"document-and-pair-wise-scores\"]).items():\n",
    "                    num = metadata[filename][\"#word\"]\n",
    "                    scores[model_id][formulation][num].append(score)\n",
    "            else:\n",
    "                print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854047f-5fd5-4175-a4b8-446e631c2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "configs = [\n",
    "    (\"nli\", 'o', \"-\"), \n",
    "    (\"pairwise\", 'x', \"--\"), \n",
    "    (\"mrc\", 's', \":\"), \n",
    "    (\"timeline\", '^', \"-.\")\n",
    "]\n",
    "colors = ['#ffb000', '#648fff']\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for model_id, color in zip(model_ids, colors):\n",
    "    formulation2nums = defaultdict(list)\n",
    "    formulation2scores = defaultdict(list)\n",
    "    for formulation, num2scores in scores[model_id].items():\n",
    "        for num, _scores in num2scores.items():\n",
    "            formulation2nums[formulation].extend([num]*len(_scores))\n",
    "            formulation2scores[formulation].extend(_scores)\n",
    "    \n",
    "    # Plotting\n",
    "    for formulation, marker, linestyle in configs:\n",
    "        x, y = np.array(formulation2nums[formulation]), np.array(formulation2scores[formulation])\n",
    "        plot = plt.scatter(x, y, marker=marker, alpha=0.2, color=color)\n",
    "        x_elements = plot.get_offsets()[:, 0]\n",
    "        x = np.array(sorted(x_elements))\n",
    "        a, b = np.polyfit(x, y, 1)\n",
    "        plt.plot(x, a*x+b, linestyle=linestyle, color=color)\n",
    "\n",
    "plt.xlabel('#word')\n",
    "plt.ylabel('F1')\n",
    "\n",
    "plt.box(False)\n",
    "plt.grid(axis='y', color='g', linestyle=':', linewidth=0.3)\n",
    "plt.ylim(0.0, 1.05)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='#ffb000', label='Llama-2-7b'),\n",
    "    Line2D([0], [0], color='#648fff', label='flan-t5-xl'),\n",
    "    Line2D([0], [0], marker='o', color='grey', label='NLI', markerfacecolor='grey', linestyle='-'),\n",
    "    Line2D([0], [0], marker='x', color='grey', label='Pairwise', markerfacecolor='grey', linestyle='--'),\n",
    "    Line2D([0], [0], marker='s', color='grey', label='MRC', markerfacecolor='grey', linestyle=':'),\n",
    "    Line2D([0], [0], marker='^', color='grey', label='Timeline', markerfacecolor='grey', linestyle='-.'),\n",
    "]\n",
    "plt.legend(handles=legend_elements, ncol=3)\n",
    "\n",
    "plt.savefig(f\"./figures/result_formulation_comparison_num_word.pdf\", format=\"pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6cf89-313d-450e-ada8-0e7f3b822699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea56d5-9130-4959-aa59-8c3f3c6f4537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbad39e-4d88-4726-aadf-ca8b680c3a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "399ba015-c822-40a6-8c73-9c8b730a9de8",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461e9cf-69e6-469e-a514-914b580c2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/preprocess/timeset-metadata/test.json\", \"r\") as f:\n",
    "    metadata_test = json.load(f)\n",
    "with open(\"../data/preprocess/timeset-metadata/dev.json\", \"r\") as f:\n",
    "    metadata_dev = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a3197-1c6c-472d-96e8-660f4af009c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = Path(\"../data/preprocess/timeset-sample/\")\n",
    "\n",
    "stats = defaultdict(int)\n",
    "stats_each = defaultdict(lambda: defaultdict(int))\n",
    "words = defaultdict(list)\n",
    "topic = defaultdict(list)\n",
    "sents = defaultdict(list)\n",
    "for split in [\"test\", \"dev\"]:\n",
    "    with open(f\"../data/preprocess/timeset-metadata/{split}.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    with open(f\"../data/preprocess/timeset-sample/{split}.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    for one_document in data:\n",
    "        events = one_document['annotation']['events']\n",
    "        relations = one_document['annotation']['relations']\n",
    "        stats['num_event'] += len(events)\n",
    "        stats_each[split]['num_event'] += len(events)\n",
    "        stats['num_relation'] += len(relations)\n",
    "        stats_each[split]['num_relation'] += len(relations)\n",
    "        stats['num_argument'] += sum([len(x['arguments']) for x in events.values()])\n",
    "        stats_each[split]['num_argument'] += sum([len(x['arguments']) for x in events.values()])\n",
    "        \n",
    "        _metadata = metadata[one_document['filename']]\n",
    "        topic[_metadata['topic'][0]].append('_'.join(_metadata['topic']))\n",
    "        words[split].append(_metadata['#word'])\n",
    "        words['all'].append(_metadata['#word'])\n",
    "        sents[split].append(_metadata['#sent'])\n",
    "        sents['all'].append(_metadata['#sent'])\n",
    "        \n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56341e03-04b3-44fd-b3a6-910cfe968617",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21726763-4510-4f12-819a-518effbf89f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(words['test'])/40, sum(words['dev'])/10, sum(words['all'])/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee4b66-4f50-43d3-b9a9-40e9b50cee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sents['test'])/40, sum(sents['dev'])/10, sum(sents['all'])/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d924422-3407-4993-99ff-d4272c4dfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39911814-6201-48a9-afe7-5e5a7c2eb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08abb25-9f14-433a-8e81-4bdf34020d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([_x.replace(\"_\", \" \") for x in topic.values() for _x in set(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab47e53-3559-4d67-a755-143c1455cff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeset",
   "language": "python",
   "name": "timeset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
