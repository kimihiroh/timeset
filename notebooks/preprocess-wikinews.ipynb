{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb8ddf7-b404-478c-9769-47dcdebc4fad",
   "metadata": {},
   "source": [
    "# Download & Preprocess Wikinews articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874c1cb-546a-411e-989b-bf93dd3f8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c5d62-61d4-4580-958b-2c21f4f0153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(split, tag):\n",
    "    \n",
    "    filepath_csv = f\"../data/wikinews/urls/urls_{split}.tsv\"\n",
    "    \n",
    "    dirpath_download = Path(f\"../data/wikinews/download/{split}\")\n",
    "    if not dirpath_download.exists():\n",
    "        dirpath_download.mkdir(parents=True)\n",
    "    \n",
    "    df = pd.read_csv(filepath_csv, sep='\\t')\n",
    "    with open(f\"../log/wikinews_download_{split}.log\", \"w\") as log:\n",
    "        for _, row in df.iterrows():\n",
    "            if not pd.isna(row[tag]):\n",
    "                topic = row['category'].strip().lower().replace(' - ','_').replace(' ', '_')\n",
    "                idx = row[\"id\"]\n",
    "                url = row[tag].strip()\n",
    "                \n",
    "                if tag == \"url\":\n",
    "                    filename = f\"{topic}_{idx}.html\"\n",
    "                elif tag == \"Other candidate\":\n",
    "                    filename = f\"{topic}_{idx}_extra.html\"\n",
    "                else:\n",
    "                    print(\"error\")\n",
    "                \n",
    "                result = subprocess.call(\n",
    "                    [\"wget\", url, \"-O\", dirpath_download / filename],\n",
    "                    stdout=log, \n",
    "                    stderr=log\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9029b2-b010-4e76-8fd8-6c892cb193bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_end_phrases = [\n",
    "    \"Have an opinion on this story?\",\n",
    "    \"Share this:\",\n",
    "    \"This page is archived, and is no longer publicly editable.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c7965-3504-4006-9eff-9b688252d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(split):\n",
    "    \n",
    "    dirpath_download = Path(f\"../data/wikinews/download/{split}\")\n",
    "    \n",
    "    dirpath_preprocess = Path(f\"../data/wikinews/preprocess/{split}\")\n",
    "    if not dirpath_preprocess.exists():\n",
    "        dirpath_preprocess.mkdir(parents=True)\n",
    "    \n",
    "    for filepath in dirpath_download.glob(\"*.html\"):\n",
    "        \n",
    "        with open(filepath, 'r') as f:\n",
    "            html_doc = f.read()\n",
    "    \n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "        text = ''\n",
    "        # title\n",
    "        text += soup.title.text.replace(' - Wikinews, the free news source', '') + '\\n'\n",
    "        \n",
    "        for p in soup.body.find_all(['p']):\n",
    "    \n",
    "            # post article irrelevant text\n",
    "            if any([True if phrase in p.text else False for phrase in article_end_phrases]):\n",
    "                break\n",
    "    \n",
    "            # remove line breaks\n",
    "            if p.text.strip() == \"\":\n",
    "                continue\n",
    "    \n",
    "            # date\n",
    "            if p.strong and p.strong.span and p.strong.span[\"id\"] == \"publishDate\":\n",
    "                text += p.text.replace(\"\\n\", \"\") + \"\\n\"\n",
    "            else:\n",
    "                text += p.text\n",
    "            \n",
    "        text = text.strip()\n",
    "        \n",
    "        with open(dirpath_preprocess / f\"{filepath.stem}.txt\", \"w\") as f:\n",
    "            f.write(text + \"\\n\")\n",
    "        with open(dirpath_preprocess / f\"{filepath.stem}.ann\", \"w\") as f:\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0762a50-1975-4c2b-b552-0c163ec1a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../log/\").mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce068c0f-8314-43e3-ac21-24f8b0fbc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"dev\", \"url\")\n",
    "preprocess(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bda6d7-b2af-47c6-bdba-1b687451e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"test\", \"url\")\n",
    "download(\"test\", \"Other candidate\")\n",
    "preprocess(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639e09d-6a80-4235-a010-d439ff4eac06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeset",
   "language": "python",
   "name": "timeset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
