#!/usr/bin/bash

#SBATCH --cpus-per-task {num_cpu}
#SBATCH --mem-per-cpu=24GB
#SBATCH --gres=gpu:A6000:{num_gpu}
#SBATCH --job-name=inf_{dataset_name}_{model_id_short}
#SBATCH --output ./log/%J-%x.log
#SBATCH --time 2-00:00:00

export CUDA_VISIBLE_DEVICES={gpu_ids}

if [ -n "$MAMBA_EXE" ]; then
  CONDA=micromamba
  eval "$(micromamba shell hook -s bash)"
else
  CONDA=conda
  eval "$(conda shell.bash hook)"
fi

$CONDA activate llm

dirpath_output=./output/benchmark/
dirpath_output_score=./output_score/benchmark/
dirpath_log=./log
filepath_test={filepath_test}
filepath_dev={filepath_dev}

dataset_name={dataset_name}
inference_type=few-shot
seeds=( 7 )

batch_size={batch_size}
max_new_tokens={max_new_tokens}
num_demonstrations=( 0 1 3 5 10 )
temperature=0
model_id={model_id}
precision_type={precision_type}
num_gpu={num_gpu}

for seed in "${seeds[@]}"; do
    for num_demonstration in "${num_demonstrations[@]}"; do

        python src/{code}.py \
            --batch_size $batch_size \
            --dataset_name $dataset_name \
            --dirpath_log $dirpath_log \
            --dirpath_output $dirpath_output \
            --dirpath_output_score $dirpath_output_score \
            --filepath_test $filepath_test \
            --filepath_dev $filepath_dev \
            --inference_type $inference_type \
            --max_new_tokens $max_new_tokens \
            --model_id $model_id \
            --num_demonstration "$num_demonstration" \
            --num_gpu $num_gpu \
            --precision_type $precision_type \
            --seed "$seed" \
            --temperature $temperature \
            --dirpath_model_cache "$TRANSFORMERS_CACHE"
    done
done
